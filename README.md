# PyLogStreams ğŸŒ€

A distributed, Redis-style **pub/sub system** with **Kafka-inspired persistent logs** â€” built for chat apps and real-time systems.

---

## ğŸŒŸ Highlights

* ğŸ” **Pub/Sub API** â€” Simple producer/consumer model
* ğŸ’¾ **Persistent logs** â€” Kafka-like segment files per topic
* âš¡ **Memory-mapped I/O** â€” High-throughput read/write
* ğŸ§¹ **Background compaction & file cleanup**
* ğŸ§  **Cluster-ready design** â€” Planned replication & partition assignment
* ğŸ§© **Modular storage engine** â€” Swap local disk for distributed backend
* ğŸ—£ï¸ Ideal for chat apps, analytics, and real-time event streaming

---

## ğŸš€ Planned Features

* [x] Persistent local log with append-only segments
* [x] Multiprocessing producer/consumer model
* [x] Background log cleaner and file remover
* [ ] Distributed cluster coordination
* [ ] Replication and leader election
* [ ] Stream rebalancing
* [ ] REST and gRPC APIs

---

## âš™ï¸ Architecture Overview

Each topic maintains an append-only commit log segmented across files.
Consumers read sequentially and track offsets.
Future versions will use **Raft or custom gossip-based coordination** for cluster management.

---

## ğŸ§ª How to Use

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/akramdevstuffs/PyLogStreams
cd PyLogStreams
```

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # Linux/macOS
venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the Broker

By default, the broker runs on **localhost:1234**.

```bash
python src/PyLogStreams/broker.py
```

You can change the `HOST` and `PORT` values in
`src/PyLogStreams/broker.py` to customize the server address.

---

### 5ï¸âƒ£ Use the Client

You can either implement your own client logic or use the built-in implementation:

```
client/client.py
```

Example:

```python
from client.client import Client

client = Client("localhost", 1234)

# Register and receive unique client ID
cid = client.register()
print("Registered with ID:", cid)

# Produce a message
client.produce("news", "Breaking: PyLogStreams is live!")

# Subscribe to a topic
client.subscribe("news")

# Consume messages
msg = client.consume()
print("Received:", msg)
```

---

### 6ï¸âƒ£ Benchmarking

To benchmark or stress-test:

```bash
python tests/test_minikafka.py
```

You can tweak producer count, message sizes, and other parameters for custom testing.

---

## ğŸ§° Default Configuration

| Setting                  | Default     |
| ------------------------ | ----------- |
| Host                     | `localhost` |
| Port                     | `1234`      |
| Heartbeat Interval       | `30s`       |
| Outgoing Buffer Capacity | `1000`      |
| Checksum                 | Enabled     |

---